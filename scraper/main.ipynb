{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following code is for the case that we converted a PDF file to html and we try to retrive the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "filename = '../pdfminer.six/results/output.html'\n",
    "html_doc = file = open(filename, \"r\").read()\n",
    "soup = BeautifulSoup(html_doc,'lxml') \n",
    "#body = soup.find('body')\n",
    "for div  in soup.find_all('div'):\n",
    "    print(div.text)\n",
    "    if \"Abstract\" in div.text.lower():\n",
    "        print(div.text)\n",
    "        abstract = div.text\n",
    "    if \"introduction\" in div.text.lower():\n",
    "        print(div.next.text)\n",
    "        intro = div.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following code is for the case that we load an html page from arxiv then we try to retrive the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import scholar as sc\n",
    "\n",
    "def read_page(file_name):\n",
    "    response = requests.get(file_name)#../pdfminer.six/results/output.html')\n",
    "    html_doc = response.text\n",
    "    return html_doc\n",
    "def extract_title(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    "    whole_box = soup.find_all('div', class_='leftcolumn')\n",
    "    for box in whole_box:\n",
    "        title = box.find('h1', class_='title mathjax').text\n",
    "    return title\n",
    "def extract_abstract(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    " #   print(soup)\n",
    "    whole_box = soup.find_all('div', class_='leftcolumn')\n",
    "    for box in whole_box:\n",
    "        query = box.find('blockquote', class_='abstract mathjax')\n",
    "        unwanted = query.find('span')\n",
    "        unwanted.extract()\n",
    "        abstract = query.text.replace(\"\\n\", \"\")\n",
    "    return abstract\n",
    "def extract_authors(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    " #   print(soup)\n",
    "    name=list()\n",
    "    address=list()\n",
    "    whole_box = soup.find_all('div', class_='authors')\n",
    "    for box in whole_box:\n",
    "        links = box.find_all('a')\n",
    "        for link in links:\n",
    "            address.append(link['href'])\n",
    "            name.append(link.text)\n",
    "    return address,name\n",
    "def extract_list_of_author_publications(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    title = list()\n",
    "    counter = 0\n",
    "    name=list()\n",
    "    address=list()\n",
    "    whole_box = soup.find_all('div', id='dlpage')\n",
    "    for box in whole_box:\n",
    "        dls = box.find_all('dl')\n",
    "        for dl in dls:\n",
    "            dds = dl.find_all('dd')\n",
    "            for dd in dds:\n",
    "                query = dd.find('div', class_='list-title mathjax')\n",
    "                unwanted = query.find('span')\n",
    "                unwanted.extract()\n",
    "                title.append(query.text.replace(\"\\n\", \"\"))\n",
    "    return title\n",
    "def extract_author_scholar_page(authors):\n",
    "    # This method only works when we want to retrive \n",
    "    querier = sc.ScholarQuerier()\n",
    "    query = sc.SearchScholarQuery()\n",
    "    query.set_author(authors)\n",
    "    querier.send_query(query)\n",
    "    name = query.get_url()\n",
    "    html_doc = read_page(name) \n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    whole_box = soup.find_all('table')\n",
    "    for box in whole_box:  \n",
    "        ali = box.find('tr')\n",
    "        print(ali)\n",
    "        input()\n",
    "        tds = ali.find_all('td')\n",
    "        a = tds[1].find('a')\n",
    "        link = a['href']\n",
    "        if link is not None:\n",
    "            return \"https://scholar.google.com\"+link\n",
    "def extract_author_scholar_info(authors):\n",
    "    # This method only works when we want to retrive \n",
    "    querier = sc.ScholarQuerier()\n",
    "    query = sc.SearchScholarQuery()\n",
    "    query.set_author(authors)\n",
    "    querier.send_query(query)\n",
    "    name = query.get_url()\n",
    "    html_doc = read_page(name) \n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    whole_box = soup.find_all('table')\n",
    "    for box in whole_box:  \n",
    "        ali = box.find('tr')\n",
    "        tds = ali.find_all('td')\n",
    "        divs = tds[1].find_all('div')\n",
    "        for div in divs:\n",
    "            if \"Verified email\" in div:\n",
    "                affil =  div.text.replace(\"Verified email at \", \"\")\n",
    "            if \"Cited\" in div:\n",
    "                cite =  div.text.replace(\"Cited by \", \"\")\n",
    "\n",
    "        if affil is not none:\n",
    "            return affil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import scholar as sc\n",
    "\n",
    "def read_page(file_name):\n",
    "    response = requests.get(file_name)#../pdfminer.six/results/output.html')\n",
    "    html_doc = response.text\n",
    "    return html_doc\n",
    "def extract_title(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    "    whole_box = soup.find_all('div', class_='leftcolumn')\n",
    "    for box in whole_box:\n",
    "        title = box.find('h1', class_='title mathjax').text\n",
    "    return title\n",
    "def extract_abstract(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    " #   print(soup)\n",
    "    whole_box = soup.find_all('div', class_='leftcolumn')\n",
    "    for box in whole_box:\n",
    "        query = box.find('blockquote', class_='abstract mathjax')\n",
    "        unwanted = query.find('span')\n",
    "        unwanted.extract()\n",
    "        abstract = query.text.replace(\"\\n\", \"\")\n",
    "    return abstract\n",
    "def extract_authors(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    #class=\"leftcolumntitle mathjax\"\n",
    "    #article = soup.\n",
    " #   title = soup.find('div', class_='title mathjax') \n",
    " #   print(soup)\n",
    "    name=list()\n",
    "    address=list()\n",
    "    whole_box = soup.find_all('div', class_='authors')\n",
    "    for box in whole_box:\n",
    "        links = box.find_all('a')\n",
    "        for link in links:\n",
    "            address.append(link['href'])\n",
    "            name.append(link.text)\n",
    "    return address,name\n",
    "def extract_list_of_author_publications(html_doc):\n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    title = list()\n",
    "    counter = 0\n",
    "    name=list()\n",
    "    address=list()\n",
    "    whole_box = soup.find_all('div', id='dlpage')\n",
    "    for box in whole_box:\n",
    "        dls = box.find_all('dl')\n",
    "        for dl in dls:\n",
    "            dds = dl.find_all('dd')\n",
    "            for dd in dds:\n",
    "                query = dd.find('div', class_='list-title mathjax')\n",
    "                unwanted = query.find('span')\n",
    "                unwanted.extract()\n",
    "                title.append(query.text.replace(\"\\n\", \"\"))\n",
    "    return title\n",
    "def extract_author_scholar_page(authors):\n",
    "    # This method only works when we want to retrive \n",
    "    querier = sc.ScholarQuerier()\n",
    "    query = sc.SearchScholarQuery()\n",
    "    query.set_author(authors)\n",
    "    querier.send_query(query)\n",
    "    name = query.get_url()\n",
    "    html_doc = read_page(name) \n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    whole_box = soup.find_all('table')\n",
    "    box = whole_box[0]    \n",
    "    ali = box.find('tr')\n",
    "    tds = ali.find_all('td')\n",
    "    a = tds[1].find('a')\n",
    "    link = a['href']\n",
    "    return \"https://scholar.google.com\"+link\n",
    "def extract_author_scholar_info(authors):\n",
    "    # This method only works when we want to retrive \n",
    "    querier = sc.ScholarQuerier()\n",
    "    query = sc.SearchScholarQuery()\n",
    "    query.set_author(authors)\n",
    "    querier.send_query(query)\n",
    "    name = query.get_url()\n",
    "    html_doc = read_page(name) \n",
    "    soup = BeautifulSoup(html_doc,'lxml')\n",
    "    whole_box = soup.find_all('table')\n",
    "    box = whole_box[0]    \n",
    "    ali = box.find('tr')\n",
    "    tds = ali.find_all('td')\n",
    "    divs = tds[1].find_all('div')\n",
    "    for div in divs:\n",
    "        if \"Verified email\" in div:\n",
    "            affil =  div.text.replace(\"Verified email at \", \"\")\n",
    "        if \"Cited\" in div:\n",
    "            cite =  div.text.replace(\"Cited by \", \"\")\n",
    "            \n",
    "            \n",
    "    return affil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"https://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\"#\"https://arxiv.org/abs/1802.00254\"\n",
    "html_doc = read_page(name)  \n",
    "title = extract_list_of_author_publications(html_doc)\n",
    "#address,name = extract_authors(html_doc)\n",
    "#print(title)\n",
    "#print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "cat_n = 1802\n",
    "max_n = 386\n",
    "len_max = 5\n",
    "for i in range(1,max_n):\n",
    "    n_zer = len_max-len(str(i))\n",
    "    z= ''\n",
    "    for j in range(1,n_zer):\n",
    "        z = z+'0'\n",
    "    name = \"https://arxiv.org/abs/\"+str(cat_n)+\".\"+z+str(i)\n",
    "    html_doc = read_page(name)   \n",
    "    titles.append(extract_title(html_doc))\n",
    "    abstracts.append(extract_abstract(html_doc))\n",
    "se = pd.Series(titles)\n",
    "se2 = pd.Series(abstracts)    \n",
    "df = pd.DataFrame(columns=['title','abstracts'])\n",
    "df['title'] = se.values\n",
    "df['abstracts'] = se2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://scholar.google.com/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=abolfazl%20razi%2Cali%20valehi&as_publication=&as_ylo=&as_yhi=&as_vis=0&btnG=&hl=en&as_sdt=0%2C5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8afc927eb33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwhole_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhole_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mali\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mali\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import scholar as sc\n",
    "AUTHORS = \"abolfazl razi,ali valehi\"\n",
    "querier = sc.ScholarQuerier()\n",
    "query = sc.SearchScholarQuery()\n",
    "query.set_author(AUTHORS)\n",
    "querier.send_query(query)\n",
    "name = query.get_url()\n",
    "print(name)\n",
    "#sc.txt(querier, '')\n",
    "#name = \"https://scholar.google.com/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=ali+valehi&as_publication=&as_ylo=&as_yhi=&hl=en&as_sdt=0%2C3\"\n",
    "html_doc = read_page(name) \n",
    "soup = BeautifulSoup(html_doc,'lxml')\n",
    "whole_box = soup.find_all('table')\n",
    "box = whole_box[0]    \n",
    "ali = box.find('tr')\n",
    "tds = ali.find_all('td')\n",
    "a = tds[1].find('a')\n",
    "link = a['href']\n",
    "return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0d19143cfe97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_author_scholar_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mohammad zaeri amirani\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-41b93f189367>\u001b[0m in \u001b[0;36mextract_author_scholar_page\u001b[0;34m(authors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mwhole_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhole_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mali\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mali\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "extract_author_scholar_page(\"mohammad zaeri amirani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(extract_author_scholar_info(\"mohammad zaeri amirani\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
